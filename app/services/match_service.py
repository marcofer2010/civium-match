"""
Servi√ßo principal de match FAISS para o Civium Match Service
"""

import asyncio
import hashlib
import time
from datetime import datetime
from typing import Dict, List, Optional, Any, Set
import os
import numpy as np
import faiss
import psutil

from app.config import settings
from app.models.api_models import (
    SmartMatchResponse,
    MatchResult,
    ServiceStats
)
from app.utils.logger import setup_logger

# Logger global para a Collection
logger = setup_logger("collection")


class Collection:
    """Collection FAISS com sistema de invalida√ß√£o local."""
    
    def __init__(self, company_id: int, company_type: str, collection_type: str):
        self.company_id = company_id
        self.company_type = company_type
        self.collection_type = collection_type
        self.index = None
        self.invalidated_positions: Set[int] = set()  # Posi√ß√µes invalidadas localmente
        self._initialize_index()
    
    @property
    def collection_key(self) -> str:
        """Chave √∫nica da collection."""
        return f"{self.company_type}_{self.company_id}_{self.collection_type}"
    
    @property
    def collection_path(self) -> str:
        """Path do arquivo FAISS no sistema."""
        return f"collections/{self.company_type}/{self.company_id}/{self.collection_type}"
    
    def _initialize_index(self):
        """Inicializar √≠ndice FAISS."""
        # IndexFlatIP para similaridade coseno (produto interno)
        self.index = faiss.IndexFlatIP(512)  # 512 dimens√µes
    
    def add_face(self, embedding: np.ndarray) -> int:
        """
        Adicionar embedding ao √≠ndice FAISS.
        
        Returns:
            index_position: Posi√ß√£o no √≠ndice FAISS (para gravar no PostgreSQL)
        """
        # Validar embedding
        if embedding.shape[0] != 512:
            raise ValueError(f"Embedding deve ter 512 dimens√µes, recebeu {embedding.shape[0]}")
        
        # Validar que n√£o √© vetor zero (m√° pr√°tica!)
        if np.allclose(embedding, 0):
            raise ValueError("Embedding n√£o pode ser vetor zero! Use invalidate_position() para invalidar.")
        
        # Normalizar para similaridade coseno
        embedding = embedding / np.linalg.norm(embedding)
        
        # Posi√ß√£o que ser√° ocupada (antes de adicionar)
        index_position = self.index.ntotal
        
        # Adicionar ao FAISS
        self.index.add(embedding.reshape(1, -1))
        
        return index_position
    
    def invalidate_position(self, index_position: int) -> bool:
        """
        Invalidar uma posi√ß√£o espec√≠fica.
        
        Em vez de alterar o FAISS (imposs√≠vel), mantemos uma lista local
        de posi√ß√µes invalidadas e filtramos nos resultados.
        
        Args:
            index_position: Posi√ß√£o no √≠ndice FAISS para invalidar
            
        Returns:
            True se invalidou com sucesso
        """
        if index_position < 0 or index_position >= self.index.ntotal:
            return False
            
        self.invalidated_positions.add(index_position)
        return True
    
    def revalidate_position(self, index_position: int) -> bool:
        """
        Revalidar uma posi√ß√£o (remover da lista de invalidadas).
        
        Args:
            index_position: Posi√ß√£o para revalidar
            
        Returns:
            True se revalidou com sucesso
        """
        if index_position in self.invalidated_positions:
            self.invalidated_positions.remove(index_position)
            return True
        return False
    
    def search(self, embedding: np.ndarray, top_k: int = 10, threshold: float = 0.4) -> List[Dict]:
        """
        Buscar embeddings similares, filtrando posi√ß√µes invalidadas.
        
        Returns:
            Lista de matches com index_position, similarity e confidence (sem invalidadas)
        """
        if self.index.ntotal == 0:
            return []
        
        # Normalizar query embedding
        embedding = embedding / np.linalg.norm(embedding)
        
        # Buscar mais resultados para compensar filtros
        search_k = min(top_k * 3, self.index.ntotal)  # 3x mais para filtrar
        similarities, indices = self.index.search(embedding.reshape(1, -1), search_k)
        
        results = []
        for i, (similarity, index_position) in enumerate(zip(similarities[0], indices[0])):
            # Filtrar: threshold, posi√ß√µes v√°lidas e N√ÉO invalidadas
            if (similarity >= threshold and 
                index_position != -1 and 
                index_position not in self.invalidated_positions):
                
                confidence = min(similarity * 100, 100.0)
                results.append({
                    "index_position": int(index_position),
                    "similarity": float(similarity),
                    "confidence": float(confidence)
                })
                
                # Parar quando tivermos resultados suficientes
                if len(results) >= top_k:
                    break
        
        return results
    
    def save_to_disk(self) -> None:
        """Salvar √≠ndice FAISS e posi√ß√µes invalidadas."""
        import os
        import pickle
        
        # Criar diret√≥rio se n√£o existir
        dir_path = f"collections/{self.company_type}/{self.company_id}"
        os.makedirs(dir_path, exist_ok=True)
        
        # Salvar √≠ndice FAISS
        index_path = f"{self.collection_path}.index"
        faiss.write_index(self.index, index_path)
        
        # Salvar posi√ß√µes invalidadas (se houver)
        if self.invalidated_positions:
            invalidated_path = f"{self.collection_path}.invalidated"
            with open(invalidated_path, 'wb') as f:
                pickle.dump(self.invalidated_positions, f)
    
    @classmethod
    def load_from_disk(cls, company_id: int, company_type: str, collection_type: str) -> 'Collection':
        """Carregar collection do disco - √≠ndice FAISS + posi√ß√µes invalidadas."""
        import pickle
        
        collection = cls(company_id, company_type, collection_type)
        
        index_path = f"{collection.collection_path}.index"
        invalidated_path = f"{collection.collection_path}.invalidated"
        
        try:
            # Carregar √≠ndice FAISS
            if os.path.exists(index_path):
                collection.index = faiss.read_index(index_path)
                
                # Carregar posi√ß√µes invalidadas
                if os.path.exists(invalidated_path):
                    with open(invalidated_path, 'rb') as f:
                        collection.invalidated_positions = pickle.load(f)
                
                valid_faces = collection.index.ntotal - len(collection.invalidated_positions)
                logger.info(f"üìÇ Collection carregada: {collection.collection_key} "
                           f"({collection.index.ntotal} total, {valid_faces} v√°lidas, "
                           f"{len(collection.invalidated_positions)} invalidadas)")
            else:
                logger.info(f"üìÇ Nova collection criada: {collection.collection_key}")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao carregar collection {collection.collection_key}: {e}")
            logger.info("üîÑ Criando nova collection...")
            collection._initialize_index()
            collection.invalidated_positions = set()
        
        return collection
    
    @property
    def face_count(self) -> int:
        """N√∫mero de faces V√ÅLIDAS na collection (total - invalidadas)."""
        total = self.index.ntotal if self.index else 0
        return total - len(self.invalidated_positions)
    
    @property  
    def total_face_count(self) -> int:
        """N√∫mero total de faces na collection (incluindo invalidadas)."""
        return self.index.ntotal if self.index else 0


class CollectionManager:
    """Gerenciador de collections com cria√ß√£o autom√°tica."""
    
    def __init__(self):
        self.logger = setup_logger("collection-manager")
        self.collections_cache: Dict[str, Collection] = {}
    
    async def get_or_create_collection(self, company_id: int, company_type: str, 
                                     collection_type: str) -> Collection:
        """Retorna collection existente ou cria nova se n√£o existir."""
        collection_key = f"{company_type}_{company_id}_{collection_type}"
        
        if collection_key not in self.collections_cache:
            # Tentar carregar do disco ou criar nova
            self.logger.info(f"üìÅ Carregando/criando collection: {collection_key}")
            collection = Collection.load_from_disk(company_id, company_type, collection_type)
            self.collections_cache[collection_key] = collection
        
        return self.collections_cache[collection_key]
    
    async def get_all_public_known_collections(self) -> List[Collection]:
        """Retorna todas as collections 'known' de √≥rg√£os p√∫blicos."""
        public_dir = "collections/public"
        collections = []
        
        if not os.path.exists(public_dir):
            return collections
        
        for company_dir in os.listdir(public_dir):
            company_path = os.path.join(public_dir, company_dir)
            if os.path.isdir(company_path):
                try:
                    collection = await self.get_or_create_collection(
                        company_id=int(company_dir),
                        company_type="public",
                        collection_type="known"
                    )
                    collections.append(collection)
                except Exception as e:
                    self.logger.error(f"‚ùå Erro ao carregar collection p√∫blica {company_dir}/known: {e}")
        
        return collections


class MatchService:
    """Servi√ßo principal de match usando FAISS."""
    
    def __init__(self):
        self.logger = setup_logger("match-service")
        self.collection_manager = CollectionManager()
        self.is_ready = False
        
        # Estat√≠sticas
        self.start_time = datetime.utcnow()
        self.stats = {
            'total_smart_matches': 0,
            'total_match_time_ms': 0,
            'auto_registrations': 0
        }
        
    async def initialize(self) -> None:
        """Inicializa o servi√ßo."""
        self.logger.info("üöÄ Inicializando Match Service...")
        
        try:
            # Criar diret√≥rio de collections se n√£o existir
            os.makedirs("collections/public", exist_ok=True)
            os.makedirs("collections/private", exist_ok=True)
            
            self.is_ready = True
            self.logger.info("‚úÖ Match Service inicializado com sucesso")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao inicializar Match Service: {e}")
            raise
    
    async def smart_match(self, embedding: List[float], company_id: int, company_type: str,
                         camera_shared: bool = False, search_unknown: bool = False,
                         auto_register: bool = False, threshold: float = None, 
                         top_k: int = None) -> SmartMatchResponse:
        """
        Busca inteligente com l√≥gica em cascata:
        1. Buscar em collections 'known' (federada se c√¢mera compartilhada)
        2. Se n√£o encontrar e search_unknown=True, buscar na collection 'unknown' da pr√≥pria empresa
        3. Se n√£o encontrar e auto_register=True, cadastrar na collection 'unknown' da pr√≥pria empresa
        """
        start_time = time.time()
        
        # Usar defaults se n√£o fornecidos
        threshold = threshold or settings.DEFAULT_MATCH_THRESHOLD
        top_k = top_k or settings.DEFAULT_TOP_K
        
        # Converter embedding para numpy
        embedding_array = np.array(embedding, dtype=np.float32)
        
        # Gerar hash do embedding para cache/debugging
        embedding_hash = hashlib.md5(embedding_array.tobytes()).hexdigest()[:16]
        
        search_details = {
            "company_id": company_id,
            "company_type": company_type,
            "camera_shared": camera_shared,
            "search_unknown": search_unknown,
            "auto_register": auto_register,
            "threshold": threshold,
            "top_k": top_k
        }
        
        collections_searched = 0
        
        # ETAPA 1: Buscar em collections 'known'
        self.logger.info(f"üîç Etapa 1: Buscando em collections 'known'...")
        
        if camera_shared:
            # BUSCA FEDERADA: C√¢mera compartilhada permite acesso a todas as collections 'known' p√∫blicas
            # Isso vale tanto para √≥rg√£os p√∫blicos quanto empresas privadas em espa√ßos compartilhados
            known_collections = await self.collection_manager.get_all_public_known_collections()
            
            # Tamb√©m incluir a pr√≥pria collection 'known' da empresa
            own_collection = await self.collection_manager.get_or_create_collection(
                company_id, company_type, "known"
            )
            known_collections.append(own_collection)
            
            self.logger.info(f"üåê BUSCA FEDERADA: {len(known_collections)} collections (p√∫blicas + pr√≥pria)")
            
        else:
            # BUSCA ISOLADA: C√¢mera privada acessa apenas pr√≥pria collection 'known'
            known_collection = await self.collection_manager.get_or_create_collection(
                company_id, company_type, "known"
            )
            known_collections = [known_collection]
            self.logger.info(f"üè¢ BUSCA ISOLADA: apenas collection pr√≥pria")
        
        collections_searched += len(known_collections)
        
        # Buscar em paralelo nas collections 'known'
        known_results, results_by_category = await self._search_multiple_collections(embedding_array, known_collections, threshold, top_k)
        
        if known_results:
            # Encontrou na collection 'known'
            best_match = known_results[0]
            search_time_ms = (time.time() - start_time) * 1000
            
            self.stats['total_smart_matches'] += 1
            self.stats['total_match_time_ms'] += search_time_ms
            
            self.logger.info(f"‚úÖ Encontrado em collection 'known': {best_match['company_id']}/{best_match['collection_type']}")
            
            # Converter results_by_category para MatchResult objects
            matches_formatted = {}
            for category, companies in results_by_category.items():
                if companies:  # S√≥ incluir se tiver companies com matches
                    matches_formatted[category] = {}
                    for company_id, matches in companies.items():
                        if matches:  # S√≥ incluir se tiver matches
                            matches_formatted[category][company_id] = [
                                MatchResult(**match) for match in matches
                            ]
            
            # Log detalhado para c√¢meras compartilhadas
            if camera_shared and len(matches_formatted) > 0:
                self.logger.info(f"üåê BUSCA FEDERADA - Resultados por categoria:")
                for category, companies in matches_formatted.items():
                    company_count = len(companies)
                    total_matches = sum(len(matches) for matches in companies.values())
                    self.logger.info(f"   üìÅ {category}: {company_count} companies, {total_matches} matches")
            
            return SmartMatchResponse(
                query_embedding_hash=embedding_hash,
                search_performed=search_details,
                result_type="found_known",
                matches=matches_formatted if matches_formatted else None,
                auto_registered_index=None,
                total_collections_searched=collections_searched,
                search_time_ms=search_time_ms,
                threshold_used=threshold,
                top_k_used=top_k
            )
        
        # ETAPA 2: Buscar na collection 'unknown' da pr√≥pria empresa (se habilitado)
        if search_unknown:
            self.logger.info(f"üîç Etapa 2: Buscando na collection 'unknown' da pr√≥pria empresa...")
            
            unknown_collection = await self.collection_manager.get_or_create_collection(
                company_id, company_type, "unknown"
            )
            collections_searched += 1
            
            unknown_results = unknown_collection.search(embedding_array, top_k, threshold)
            
            if unknown_results:
                # Encontrou na collection 'unknown'
                best_match = unknown_results[0]
                search_time_ms = (time.time() - start_time) * 1000
                
                self.stats['total_smart_matches'] += 1
                self.stats['total_match_time_ms'] += search_time_ms
                
                self.logger.info(f"‚úÖ Encontrado em collection 'unknown': {company_id}")
                
                # Organizar o match na nova estrutura por categoria
                category = "public" if company_type == "public" else "private"
                matches_formatted = {
                    category: {
                        str(company_id): [MatchResult(
                            index_position=best_match["index_position"],
                            similarity=best_match["similarity"],
                            confidence=best_match["confidence"]
                        )]
                    }
                }
                
                return SmartMatchResponse(
                    query_embedding_hash=embedding_hash,
                    search_performed=search_details,
                    result_type="found_unknown",
                    matches=matches_formatted if matches_formatted else None,
                    auto_registered_index=None,
                    total_collections_searched=collections_searched,
                    search_time_ms=search_time_ms,
                    threshold_used=threshold,
                    top_k_used=top_k
                )
        
        # ETAPA 3: Auto-registro na collection 'unknown' (se habilitado)
        if auto_register:
            self.logger.info(f"ü§ñ Etapa 3: Auto-registrando na collection 'unknown'...")
            
            index_position = await self.add_face_to_collection(
                company_id=company_id,
                company_type=company_type,
                collection_type="unknown",
                embedding=embedding
            )
            
            search_time_ms = (time.time() - start_time) * 1000
            
            self.stats['total_smart_matches'] += 1
            self.stats['auto_registrations'] += 1
            self.stats['total_match_time_ms'] += search_time_ms
            
            self.logger.info(f"‚úÖ Face auto-registrada na posi√ß√£o: {index_position}")
            
            return SmartMatchResponse(
                query_embedding_hash=embedding_hash,
                search_performed=search_details,
                result_type="auto_registered",
                matches=None,
                auto_registered_index=index_position,
                total_collections_searched=collections_searched,
                search_time_ms=search_time_ms,
                threshold_used=threshold,
                top_k_used=top_k
            )
        
        # N√£o encontrou em lugar nenhum
        search_time_ms = (time.time() - start_time) * 1000
        
        self.stats['total_smart_matches'] += 1
        self.stats['total_match_time_ms'] += search_time_ms
        
        self.logger.info(f"‚ùå N√£o encontrado em nenhuma collection")
        
        return SmartMatchResponse(
            query_embedding_hash=embedding_hash,
            search_performed=search_details,
            result_type="not_found",
            matches=None,
            auto_registered_index=None,
            total_collections_searched=collections_searched,
            search_time_ms=search_time_ms,
            threshold_used=threshold,
            top_k_used=top_k
        )
    
    async def _search_multiple_collections(self, embedding: np.ndarray, collections: List[Collection],
                                         threshold: float, top_k: int) -> tuple[List[Dict], Dict[str, Dict[str, List[Dict]]]]:
        """
        Busca em m√∫ltiplas collections em paralelo.
        
        Returns:
            tuple: (consolidated_results, results_by_category)
                - consolidated_results: Top results consolidados ordenados por similaridade
                - results_by_category: Resultados organizados por categoria (public/private) e company
        """
        if not collections:
            return [], {}
        
        # Buscar em paralelo (limitando concorr√™ncia)
        semaphore = asyncio.Semaphore(5)
        
        async def search_one(collection):
            async with semaphore:
                results = collection.search(embedding, top_k, threshold)
                return collection, results
        
        tasks = [search_one(collection) for collection in collections]
        results_list = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Organizar resultados por categoria (public/private) e company
        results_by_category = {"public": {}, "private": {}}
        all_results = []
        
        for result in results_list:
            if isinstance(result, tuple) and len(result) == 2:
                collection, matches = result
                
                # Determinar categoria baseada no tipo da empresa
                category = "public" if collection.company_type == "public" else "private"
                company_id = collection.company_id
                
                # Simplificar matches (usar apenas dados do FAISS)
                simplified_matches = []
                for match in matches:
                    simplified_matches.append({
                        "index_position": match["index_position"],
                        "similarity": match["similarity"], 
                        "confidence": match["confidence"]
                    })
                
                # Armazenar por categoria e company (converter company_id para string)
                company_id_str = str(company_id)
                if company_id_str not in results_by_category[category]:
                    results_by_category[category][company_id_str] = []
                results_by_category[category][company_id_str].extend(simplified_matches)
                
                # Adicionar ao consolidado (manter estrutura original para compatibilidade interna)
                all_results.extend(matches)
        
        # Ordenar consolidado por similaridade (maior primeiro)
        all_results.sort(key=lambda x: x['similarity'], reverse=True)
        
        return all_results[:top_k], results_by_category
    
    async def add_face_to_collection(self, company_id: int, company_type: str, collection_type: str,
                                    embedding: List[float]) -> int:
        """Adiciona uma face a uma collection."""
        collection = await self.collection_manager.get_or_create_collection(
            company_id, company_type, collection_type
        )
        
        # Converter embedding para numpy
        embedding_array = np.array(embedding, dtype=np.float32)
        
        # Adicionar √† collection e obter posi√ß√£o
        index_position = collection.add_face(embedding_array)
        
        # Salvar altera√ß√µes
        collection.save_to_disk()
        
        self.logger.info(f"üë§ Face adicionada na posi√ß√£o {index_position} √† collection {collection.collection_key}")
        
        return index_position
    
    async def remove_face_from_collection(self, company_id: int, company_type: str, 
                                        collection_type: str, index_position: int) -> bool:
        """
        Remove (invalida) uma face de uma collection.
        
        Usa invalida√ß√£o local: mant√©m o embedding no FAISS mas filtra dos resultados.
        Para remo√ß√£o definitiva, voc√™ deve fazer no PostgreSQL.
        """
        try:
            collection = await self.collection_manager.get_or_create_collection(
                company_id, company_type, collection_type
            )
            
            # Invalidar localmente
            success = collection.invalidate_position(index_position)
            
            if success:
                # Salvar altera√ß√µes (incluindo lista de invalidadas)
                collection.save_to_disk()
                
                self.logger.info(f"‚úÖ Face na posi√ß√£o {index_position} invalidada em {company_type}/{company_id}/{collection_type}")
                self.logger.info(f"üí° Total: {collection.total_face_count}, V√°lidas: {collection.face_count}, Invalidadas: {len(collection.invalidated_positions)}")
                
                return True
            else:
                self.logger.warning(f"‚ùå Posi√ß√£o {index_position} inv√°lida ou fora do range em {company_type}/{company_id}/{collection_type}")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao invalidar face na posi√ß√£o {index_position}: {e}")
            return False
    
    async def get_stats(self) -> ServiceStats:
        """Retorna estat√≠sticas do servi√ßo."""
        uptime = (datetime.utcnow() - self.start_time).total_seconds()
        
        avg_match_time = 0
        if self.stats['total_smart_matches'] > 0:
            avg_match_time = self.stats['total_match_time_ms'] / self.stats['total_smart_matches']
        
        # Informa√ß√µes de mem√≥ria
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        # Contar faces v√°lidas vs totais
        total_faces = 0
        valid_faces = 0
        invalidated_faces = 0
        
        for collection in self.collection_manager.collections_cache.values():
            total_faces += collection.total_face_count
            valid_faces += collection.face_count
            invalidated_faces += len(collection.invalidated_positions)
        
        return ServiceStats(
            uptime_seconds=uptime,
            total_smart_matches=self.stats['total_smart_matches'],
            total_collections=len(self.collection_manager.collections_cache),
            total_faces=valid_faces,  # Mostrar apenas faces v√°lidas no total
            average_match_time_ms=avg_match_time,
            auto_registrations=self.stats['auto_registrations'],
            memory_usage_mb=memory_mb,
            # Adicionar informa√ß√µes extras para debug
            total_faces_including_invalidated=total_faces,
            invalidated_faces=invalidated_faces
        )
    
    async def cleanup(self) -> None:
        """Limpa recursos do servi√ßo."""
        self.logger.info("üßπ Limpando recursos do Match Service...")
        
        # Salvar todas as collections
        for collection in self.collection_manager.collections_cache.values():
            try:
                collection.save_to_disk()
            except Exception as e:
                self.logger.error(f"‚ùå Erro ao salvar collection {collection.collection_key}: {e}")
        
        self.logger.info("‚úÖ Cleanup conclu√≠do")
    
    # NOTA: M√©todos promote/demote removidos
    # Com FAISS puro, promo√ß√£o/rebaixamento deve ser feita:
    # 1. Obter embedding da posi√ß√£o original via PostgreSQL
    # 2. Adicionar na nova collection  
    # 3. Marcar original como removida no PostgreSQL

    # Novos m√©todos para trabalhar com paths de collections
    
    def _parse_collection_path(self, collection_path: str) -> tuple[str, int, str]:
        """
        Fazer parse do path da collection.
        
        Args:
            collection_path: Path no formato "company_type/company_id/collection_type"
            
        Returns:
            tuple: (company_type, company_id, collection_type)
        """
        parts = collection_path.split('/')
        if len(parts) != 3:
            raise ValueError('Path deve ter formato: company_type/company_id/collection_type')
        
        company_type, company_id_str, collection_type = parts
        
        if company_type not in ['public', 'private']:
            raise ValueError('company_type deve ser "public" ou "private"')
        
        try:
            company_id = int(company_id_str)
        except ValueError:
            raise ValueError('company_id deve ser um n√∫mero')
            
        if collection_type not in ['known', 'unknown']:
            raise ValueError('collection_type deve ser "known" ou "unknown"')
            
        return company_type, company_id, collection_type

    async def add_face_by_path(self, collection_path: str, embedding: List[float]) -> int:
        """
        Adicionar face usando path da collection.
        
        Args:
            collection_path: Path no formato "company_type/company_id/collection_type"
            embedding: Embedding da face
            
        Returns:
            Posi√ß√£o no √≠ndice FAISS
        """
        company_type, company_id, collection_type = self._parse_collection_path(collection_path)
        
        return await self.add_face_to_collection(
            company_id=company_id,
            company_type=company_type,
            collection_type=collection_type,
            embedding=embedding
        )

    async def remove_face_by_path(self, collection_path: str, index_position: int) -> bool:
        """
        Remover face usando path da collection.
        
        Args:
            collection_path: Path no formato "company_type/company_id/collection_type"
            index_position: Posi√ß√£o no √≠ndice FAISS
            
        Returns:
            True se removido com sucesso
        """
        company_type, company_id, collection_type = self._parse_collection_path(collection_path)
        
        return await self.remove_face_from_collection(
            company_id=company_id,
            company_type=company_type,
            collection_type=collection_type,
            index_position=index_position
        )

    async def transfer_face(self, origin_path: str, target_path: str, index_position: int) -> Dict:
        """
        Transferir face entre collections.
        
        NOTA: Com FAISS puro, voc√™ deve:
        1. Obter embedding da posi√ß√£o original via PostgreSQL (n√£o do FAISS)
        2. Adicionar na nova collection
        3. Marcar original como removida no PostgreSQL
        
        Args:
            origin_path: Path da collection de origem
            target_path: Path da collection de destino
            index_position: Posi√ß√£o no √≠ndice FAISS da collection de origem
            
        Returns:
            Dicion√°rio com informa√ß√µes da transfer√™ncia
        """
        self.logger.error(f"‚ùå Transfer n√£o suportado com FAISS puro!")
        self.logger.error(f"üí° Implemente via PostgreSQL:")
        self.logger.error(f"   1. SELECT embedding FROM faces WHERE index_position = {index_position}")
        self.logger.error(f"   2. POST /api/v2/faces com embedding + {target_path}")
        self.logger.error(f"   3. UPDATE faces SET removed = true WHERE index_position = {index_position}")
        
        raise ValueError(
            "Transfer n√£o suportado com FAISS puro. "
            "Use PostgreSQL para obter embedding e adicionar na nova collection."
        ) 